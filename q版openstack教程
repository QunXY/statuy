1，注意：本教程为openstack q版安装教程，其中密码默认使用官方文档中的密码，生产中建议替换掉，另外数据库没用设定root密码，
如果使用请按照生产标准配置密码。
2，配置需求：
控制节点默认为controller，计算节点为compute
controller节点建议2核4g以上
compute节点1核2g即可
本教程：
controller节点ip为192.168.112.100
compute节点ip为192.168.75.16
请在安装过程中涉及到相关节点ip按照使用者的ip进行替换。
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
3，准备openstack q版的源：
准备阿里base源
yum list | grep openstack 然后yum install 相关版本release -y
#curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
yum install centos-release-openstack-queens.noarch -y
安装完成后会在repo仓库里生成q版的配置源仓库，建议yum makecache 
如果系统是中文可能会出现acii码报错：
#acii码的问题解决方法：
在/etc/environment文件中添加定义 
LANG=en_US.utf-8
LC_ALL=en_US.utf-8
4，
基础环境准备工作
更改节点名字：
vim /etc/hosts
192.168.112.100 controller
192.168.75.16 compute
hostnamectl set-hostname controller
hostnamectl set-hostname compute
断开shell重进，否则会有节点名字不符导致消息队列链接的问题。
安装基础服务：
所有节点安装：
yum install chrony -y
控制节点
（选择性更改ntp服务器上游，此处不做更改）
vim /etc/chrony.conf
26行 allow 192.168.0.0/16
systemctl restart chronyd
计算节点：
vim /etc/chronyd.conf
修改3行
server 192.168.75.18 iburst
剩下server注释掉

所有节点同时安装：
安装OpenStack客户端和OpenStack-selinux
yum install python-openstackclient openstack-selinux -y
在controller节点安装：
安装mariadb：
yum install mariadb mariadb-server python2-PyMySQL -y
增加mariadb的openstack配置：
echo '[mysqld]
bind-address = 192.168.112.100
default-storage-engine = innodb
innodb_file_per_table = on #设置每个表的独立表空间文件
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8' >/etc/my.cnf.d/openstack.cnf

systemctl start mariadb
systemctl enable mariadb
mysql_secure_installation  #数据库安全初始化，不做会同步有问题
回车
n #不设置数据库密码
y
y
y
y
直接创建核心组件所需数据库以及使用用户及密码：
kestone相关：
create database keystone;
grant all on keystone.* to 'keystone'@'localhost' identified by 'KEYSTONE_DBPASS';
grant all on keystone.* to 'keystone'@'%' identified by 'KEYSTONE_DBPASS';
glance相关：
create database glance;
grant all on glance.* to 'glance'@'localhost' identified by 'GLANCE_DBPASS';
grant all on glance.* to 'glance'@'%' identified by 'GLANCE_DBPASS';
nova相关：
create database nova;
grant all on nova.* to 'nova'@'localhost' identified by 'NOVA_DBPASS';
grant all on nova.* to 'nova'@'%' identified by 'NOVA_DBPASS';
nova api相关：
create database nova_api;
grant all on nova_api.* to 'nova'@'localhost' identified by 'NOVA_DBPASS';
grant all on nova_api.* to 'nova'@'%' identified by 'NOVA_DBPASS';
nova_cell0相关（q版新增）：
CREATE DATABASE nova_cell0;
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
  IDENTIFIED BY 'NOVA_DBPASS';
neutron相关：
create database neutron;
grant all on neutron.* to 'neutron'@'localhost' identified by 'NEUTRON_DBPASS';
grant all on neutron.* to 'neutron'@'%' identified by 'NEUTRON_DBPASS';
查看创建用户
select user,host from mysql.user;

跳过nosql（涉及计费服务）

安装消息队列
yum install rabbitmq-server -y
systemctl start rabbitmq-server.service
systemctl enable rabbitmq-server.service
rabbitmqctl add_user openstack RABBIT_PASS  （增加OpenStack用户设置密码为RABBIT_PASS）
rabbitmqctl set_permissions openstack ".*" ".*" ".*"  （可读可写可配置）
rabbitmq-plugins enable rabbitmq_management (启用rabbitmq管理插件）默认登录账号guest 密码guest
如果更改host没用断开再重进，导致节点名字命bash变化引起rabbit不可用的情况，请按照以下步骤修复：
#将mq的进程杀掉
#ps -ef | grep rabbitmq | grep -v grep | awk '{print $2}' | xargs kill -9
#启动mq
#rabbitmq-server -detached
#查询mq的状态
#rabbitmqctl status
验证rabbitmq：
http://192.168.75.21:15672/   guest guest

安装memcached（注意更改memcached的监听ip默认是127.0.0.1，此处更改为0.0.0.0生产慎重更改）
yum install memcached python-memcached -y
sed -i "s#127.0.0.1#0.0.0.0#g" /etc/sysconfig/memcached  （或者改成本机ip）
systemctl start memcached
systemctl enable memcached
安装(etcd)：
yum install etcd -y
编辑 
vim /etc/etcd/etcd.conf
#[Member]
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.112.100:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.112.100:2379"
ETCD_NAME="controller"
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.112.100:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.112.100:2379"
ETCD_INITIAL_CLUSTER="controller=http://192.168.112.100:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"
ETCD_INITIAL_CLUSTER_STATE="new"
启动并开机自启etcd：
systemctl enable etcd
systemctl start etcd
以上为基础服务 	
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
5，keystone认证服务：
通过阿帕奇启动：
认证管理，授权管理，服务目录
认证：账号密码
授权：授权管理
服务目录： 记录作用记录每个服务的相关信息
yum install openstack-utils -y （自动改配置文件工具建议所有节点安装此包后面配置生产都是用的他）
yum install openstack-keystone httpd mod_wsgi -y

更改keystone配置：
openstack-config --set /etc/keystone/keystone.conf database connection  mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone
openstack-config --set /etc/keystone/keystone.conf token provider  fernet
同步数据库：
su -s /bin/sh -c "keystone-manage db_sync" keystone

初始化fernet：
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone

q版的keystone初始化认证，去掉35357认证，通过5000可以全部认证：
keystone-manage bootstrap --bootstrap-password ADMIN_PASS \
  --bootstrap-admin-url http://controller:5000/v3/ \
  --bootstrap-internal-url http://controller:5000/v3/ \
  --bootstrap-public-url http://controller:5000/v3/ \
  --bootstrap-region-id RegionOne
配置httpd：
echo "ServerName controller"  >> /etc/httpd/conf/httpd.conf  (优化阿帕奇）
q版需要创建软连接：
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
进行优化，启动阿帕奇
systemctl start httpd.service
systemctl enable httpd.service

创建注册账户，注册keystone自己api：
声明安装注册参数：
export OS_USERNAME=admin
export OS_PASSWORD=ADMIN_PASS
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
q版创建域（地区）、项目（租户）、用户、角色 因为q版开始在初始化验证部分生成了default域，此处无需创建：
#只需创建service服务项即可：
openstack project create --domain default \
  --description "Service Project" service
测试keystone服务：
openstack token issue
环境变量退出终端变量会消失。
家目录下创建环境变量脚本：
echo 'export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=ADMIN_PASS
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2' >/root/admin-openrc
source ~/admin-openrc
直接写到，.bashrc中
echo 'source admin-openrc' >> ~/.bashrc
再次验证：
openstack user list
openstack token issue
无报错则keystone正常。
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
6，glance相关：（q版没有过多改动）
在keystone上创建glance相关关联角色：
openstack user create --domain default --password GLANCE_PASS glance
openstack role add --project service --user glance admin
验证：openstack role assignment list
openstack user list （可以与上面的表对应）
openstack project list （与上面一样）
在keystone上创建服务和api
openstack service create --name glance   --description "OpenStack Image" image
openstack endpoint create --region RegionOne   image public http://controller:9292
openstack endpoint create --region RegionOne   image internal http://controller:9292
openstack endpoint create --region RegionOne   image admin http://controller:9292
安装glance服务

yum install openstack-glance -y
修改glance配置文件：

#用openstack配置命令生成（glanceapi的文件）
openstack-config --set /etc/glance/glance-api.conf  database  connection  mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
openstack-config --set /etc/glance/glance-api.conf  glance_store stores  file,http
openstack-config --set /etc/glance/glance-api.conf  glance_store default_store  file
openstack-config --set /etc/glance/glance-api.conf  glance_store filesystem_store_datadir  /var/lib/glance/images/
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_url  http://controller:5000 #因为q版keystone验证改为全部5000端口所以此处有更改
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken project_name  service
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken username  glance
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken password  GLANCE_PASS
openstack-config --set /etc/glance/glance-api.conf  paste_deploy flavor  keystone
#用openstack配置命令生成（glanceregistry文件）
openstack-config --set /etc/glance/glance-registry.conf  database  connection  mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_url  http://controller:5000 #同上
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken project_name  service
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken username  glance
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken password  GLANCE_PASS
openstack-config --set /etc/glance/glance-registry.conf  paste_deploy flavor  keystone
同步数据库：
su -s /bin/sh -c "glance-manage db_sync" glance
启动服务：
systemctl start openstack-glance-api.service openstack-glance-registry.service
systemctl enable openstack-glance-api.service openstack-glance-registry.service
上传镜像测试：（镜像请自行下载）
上传命令：openstack image create "cirros"（名字） --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 （格式）\
--container-format bare（说明是openstack的镜像） --public （指定为公共镜像）
查看相关镜像：
openstack image list

openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 \
--container-format bare --public

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

7，nova的计算服务（核心服务）
#在controller节点操作：
#keystone创建系统用户：
openstack user create --domain default --password NOVA_PASS nova
openstack role add --project service --user nova admin
#在keystone上注册服务和api
openstack service create --name nova   --description "OpenStack Compute" compute
openstack endpoint create --region RegionOne   compute public http://controller:8774/v2.1
openstack endpoint create --region RegionOne   compute internal http://controller:8774/v2.1
openstack endpoint create --region RegionOne   compute admin http://controller:8774/v2.1
#创建placement api相关：
#在keystone创建系统用户：
openstack user create --domain default --password PLACEMENT_PASS placement
openstack role add --project service --user placement admin
#在keystone上注册服务和api
openstack service create --name placement --description "Placement API" placement
openstack endpoint create --region RegionOne placement public http://controller:8778
openstack endpoint create --region RegionOne placement internal http://controller:8778
openstack endpoint create --region RegionOne placement admin http://controller:8778
#在controller节点安装nova服务：
yum install openstack-nova-api openstack-nova-conductor \
  openstack-nova-console openstack-nova-novncproxy \
  openstack-nova-scheduler openstack-nova-placement-api -y
#生成配置文件：
openstack-config --set /etc/nova/nova.conf  DEFAULT enabled_apis  osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf  DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/nova/nova.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/nova/nova.conf  DEFAULT my_ip  192.168.112.100
openstack-config --set /etc/nova/nova.conf  DEFAULT use_neutron  True
openstack-config --set /etc/nova/nova.conf  DEFAULT firewall_driver  nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf  api_database connection  mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api
openstack-config --set /etc/nova/nova.conf  database  connection  mysql+pymysql://nova:NOVA_DBPASS@controller/nova
openstack-config --set /etc/nova/nova.conf  glance api_servers  http://controller:9292
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_url  http://controller:5000/v3
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  memcached_servers  controller:11211
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_type  password
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  user_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_name  service
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  username  nova
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  password  NOVA_PASS
openstack-config --set /etc/nova/nova.conf  oslo_concurrency lock_path  /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf  libvirt  virt_type qemu
openstack-config --set /etc/nova/nova.conf  vnc vncserver_listen  '$my_ip'
openstack-config --set /etc/nova/nova.conf  vnc vncserver_proxyclient_address  '$my_ip'
openstack-config --set /etc/nova/nova.conf  placement os_region_name RegionOne
openstack-config --set /etc/nova/nova.conf  placement project_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement user_domain_name  Default
openstack-config --set /etc/nova/nova.conf  placement project_name  service
openstack-config --set /etc/nova/nova.conf  placement auth_type  password
openstack-config --set /etc/nova/nova.conf  placement auth_url  http://controller:5000/v3
openstack-config --set /etc/nova/nova.conf  placement username  placement
openstack-config --set /etc/nova/nova.conf  placement password  PLACEMENT_PASS
openstack-config --set /etc/nova/nova.conf  neutron url  http://controller:9696
openstack-config --set /etc/nova/nova.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  neutron auth_type  password
openstack-config --set /etc/nova/nova.conf  neutron project_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron user_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  neutron project_name  service
openstack-config --set /etc/nova/nova.conf  neutron username  neutron
openstack-config --set /etc/nova/nova.conf  neutron password  NEUTRON_PASS
openstack-config --set /etc/nova/nova.conf  neutron service_metadata_proxy  True
openstack-config --set /etc/nova/nova.conf  neutron metadata_proxy_shared_secret  METADATA_SECRET

#更改placement httpd设置
vim /etc/httpd/conf.d/00-nova-placement-api.conf:
#添加：
<Directory /usr/bin>
   <IfVersion >= 2.4>
      Require all granted
   </IfVersion>
   <IfVersion < 2.4>
      Order allow,deny
      Allow from all
   </IfVersion>
</Directory>
#重启httpd
systemctl restart httpd
#同步数据库：
 #nova-api：
 su -s /bin/sh -c "nova-manage api_db sync" nova
 #cell0：
 su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
 #生成cell1 cell:
 su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
 #会生成一个id用于验证：2ba0602f-ca29-49ab-9bf4-86c594ad9963   b91ac78a-e596-4ffd-a3ca-f87e43c34879
 #nova：
 su -s /bin/sh -c "nova-manage db sync" nova
 #验证nova cell0 and cell1：
 nova-manage cell_v2 list_cells
 #会返回id值
 #启动开机自启并服务：
 systemctl enable openstack-nova-api.service \
  openstack-nova-consoleauth.service openstack-nova-scheduler.service \
  openstack-nova-conductor.service openstack-nova-novncproxy.service openstack-nova-console

systemctl start openstack-nova-api.service \
  openstack-nova-consoleauth.service openstack-nova-scheduler.service \
  openstack-nova-conductor.service openstack-nova-novncproxy.service


验证：openstack compute service list
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
计算（compute）节点安装nova服务：
yum install openstack-nova-compute -y
yum install openstack-utils -y
#报错解决：
如果出现Requires: qemu-kvm-rhev >= 2.10.0
在base源里增加：
[Virt]
name=CentOS-$releasever - Base
baseurl=http://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
#配置：
openstack-config --set /etc/nova/nova.conf  DEFAULT enabled_apis  osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/nova/nova.conf  DEFAULT my_ip  192.168.75.16
openstack-config --set /etc/nova/nova.conf  DEFAULT use_neutron  True
openstack-config --set /etc/nova/nova.conf  DEFAULT firewall_driver  nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf  glance api_servers  http://controller:9292
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_url  http://controller:5000/v3
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  memcached_servers  controller:11211
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_type  password
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  user_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_name  service
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  username  nova
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  password  NOVA_PASS
openstack-config --set /etc/nova/nova.conf  oslo_concurrency lock_path  /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf  DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/nova/nova.conf  vnc enabled  True
openstack-config --set /etc/nova/nova.conf  vnc vncserver_listen  0.0.0.0
openstack-config --set /etc/nova/nova.conf  vnc vncserver_proxyclient_address  '$my_ip'
openstack-config --set /etc/nova/nova.conf  vnc novncproxy_base_url  http://controller:6080/vnc_auto.html
openstack-config --set /etc/nova/nova.conf  placement os_region_name RegionOne
openstack-config --set /etc/nova/nova.conf  placement project_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement user_domain_name  Default
openstack-config --set /etc/nova/nova.conf  placement project_name  service
openstack-config --set /etc/nova/nova.conf  placement auth_type  password
openstack-config --set /etc/nova/nova.conf  placement auth_url  http://controller:5000/v3
openstack-config --set /etc/nova/nova.conf  placement username  placement
openstack-config --set /etc/nova/nova.conf  placement password  PLACEMENT_PASS
openstack-config --set /etc/nova/nova.conf  libvirt  virt_type  qemu
#openstack-config --set /etc/nova/nova.conf  scheduler discover_hosts_in_cells_interval 300 #不明确是在compute节点还算在controller加，不加不影响使用。
启动并开机自启服务：
systemctl enable libvirtd.service openstack-nova-compute.service
systemctl start libvirtd.service openstack-nova-compute.service

注意：
回到controller节点：
#验证：
openstack compute service list --service nova-compute
#创建compute节点网格：
su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
#如果增加新节点，可在/etc/nova/nova.conf增加配置（此处不是必备步骤）
#discover_hosts_in_cells_interval = 300
验证：
openstack compute service list
openstack catalog list
openstack image list
nova-status upgrade check

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
8，neutron网络服务安装：
（此处为平面网络未涉及vxlan）
在controller节点：
创建用户关联角色
openstack user create --domain default --password NEUTRON_PASS neutron
openstack role add --project service --user neutron admin
创建服务注册api
openstack service create --name neutron   --description "OpenStack Networking" network
openstack endpoint create --region RegionOne   network public http://controller:9696
openstack endpoint create --region RegionOne   network internal http://controller:9696
openstack endpoint create --region RegionOne   network admin http://controller:9696
安装相关包：
yum install openstack-neutron openstack-neutron-ml2 \
  openstack-neutron-linuxbridge ebtables ipset -y
需要内核开启br_netfilter模块，方法如下：
#在/etc/sysctl.conf中添加：
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1 
/sbin/sysctl -p时会报错没用这个文件或目录，解决方法：
modprobe br_netfilter
/sbin/sysctl -p
#开机模块会消失，解决方法：
在/etc/新建rc.sysinit 文件
cat /etc/rc.sysinit
#!/bin/bash
for file in /etc/sysconfig/modules/*.modules ; do
[ -x $file ] && $file
done
#在/etc/sysconfig/modules/目录下新建文件如下
cat /etc/sysconfig/modules/br_netfilter.modules
modprobe br_netfilter
vim /etc/sysconfig/modules/br_netfilter.modules
增加内容：
modprobe br_netfilter
#增加权限
chmod 755 /etc/sysconfig/modules/br_netfilter.modules
如果出现PMD: net_mlx4: cannot load glue library: libibverbs.so.1: cannot open shared object file: No such file or directory错误修复方法：
解决方法：
yum install -y libibverbs
修改neutron相应配置：
openstack-config --set /etc/neutron/neutron.conf  DEFAULT core_plugin  ml2
openstack-config --set /etc/neutron/neutron.conf  DEFAULT service_plugins
openstack-config --set /etc/neutron/neutron.conf  DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/neutron/neutron.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/neutron/neutron.conf  DEFAULT notify_nova_on_port_status_changes  True
openstack-config --set /etc/neutron/neutron.conf  DEFAULT notify_nova_on_port_data_changes  True
openstack-config --set /etc/neutron/neutron.conf  database connection  mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_name  service
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken username  neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken password  NEUTRON_PASS
openstack-config --set /etc/neutron/neutron.conf  nova auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  nova auth_type  password 
openstack-config --set /etc/neutron/neutron.conf  nova project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  nova user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  nova region_name  RegionOne
openstack-config --set /etc/neutron/neutron.conf  nova project_name  service
openstack-config --set /etc/neutron/neutron.conf  nova username  nova
openstack-config --set /etc/neutron/neutron.conf  nova password  NOVA_PASS
openstack-config --set /etc/neutron/neutron.conf  oslo_concurrency lock_path  /var/lib/neutron/tmp
#cat ml2_conf.ini >/etc/neutron/plugins/ml2/ml2_conf.ini 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 type_drivers  flat,vlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 tenant_network_types 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 mechanism_drivers  linuxbridge
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 extension_drivers  port_security
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2_type_flat flat_networks  provider
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  securitygroup enable_ipset  True
#cat linuxbridge_agent.ini >/etc/neutron/plugins/ml2/linuxbridge_agent.ini 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  linux_bridge physical_interface_mappings  provider:ens33
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup enable_security_group  True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup firewall_driver  neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  vxlan enable_vxlan  False
#cat dhcp_agent.ini >/etc/neutron/dhcp_agent.ini 
openstack-config --set /etc/neutron/dhcp_agent.ini  DEFAULT interface_driver linuxbridge
openstack-config --set /etc/neutron/dhcp_agent.ini  DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set /etc/neutron/dhcp_agent.ini  DEFAULT enable_isolated_metadata true
#cat metadata_agent.ini >/etc/neutron/metadata_agent.ini 
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_host  controller
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret  METADATA_SECRET
#在nova中增加neutron的相关配置：
openstack-config --set /etc/nova/nova.conf  neutron url  http://controller:9696
openstack-config --set /etc/nova/nova.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  neutron auth_type  password
openstack-config --set /etc/nova/nova.conf  neutron project_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron user_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  neutron project_name  service
openstack-config --set /etc/nova/nova.conf  neutron username  neutron
openstack-config --set /etc/nova/nova.conf  neutron password  NEUTRON_PASS
openstack-config --set /etc/nova/nova.conf  neutron service_metadata_proxy  True
openstack-config --set /etc/nova/nova.conf  neutron metadata_proxy_shared_secret  METADATA_SECRET
#做软连接找到linux_bridge插件
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
#同步数据库：
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
#重启nova-api
systemctl restart openstack-nova-api.service
#启动并开机自启neutron服务：
systemctl enable neutron-server.service \
  neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
  neutron-metadata-agent.service
systemctl start neutron-server.service \
  neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
  neutron-metadata-agent.service
验证：
openstack network agent list
排错思路以及方法：
清空某目录(/var/log为例)下所有.log文件的日志：
find /var/log/ -type f | awk '{print ">" $0}' | bash
查找所有日志中error关键字：
find /var/log/ -type f | xargs grep -i error
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
在计算节点（compute）：

yum install  openstack-neutron-linuxbridge ebtables ipset -y
需要内核开启br_netfilter模块，方法如下：
在/etc/sysctl.conf中添加：
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1 
/sbin/sysctl -p时会报错没用这个文件或目录，解决方法：
modprobe br_netfilter
/sbin/sysctl -p
开机模块会消失，解决方法：
在/etc/新建rc.sysinit 文件
vim /etc/rc.sysinit
#!/bin/bash
for file in /etc/sysconfig/modules/*.modules ; do
[ -x $file ] && $file
done
在/etc/sysconfig/modules/目录下新建文件如下
vim /etc/sysconfig/modules/br_netfilter.modules
modprobe br_netfilter
增加权限
chmod 755 /etc/sysconfig/modules/br_netfilter.modules
更改neutron配置：
openstack-config --set /etc/neutron/neutron.conf  DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/neutron/neutron.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_name  service
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken username  neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken password  NEUTRON_PASS
openstack-config --set /etc/neutron/neutron.conf  oslo_concurrency lock_path  /var/lib/neutron/tmp


#cat linuxbridge_agent.ini >/etc/neutron/plugins/ml2/linuxbridge_agent.ini 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  linux_bridge physical_interface_mappings  provider:ens33
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup enable_security_group  True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup firewall_driver  neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  vxlan enable_vxlan  False
#增加nova配置：
openstack-config --set /etc/nova/nova.conf  neutron url  http://controller:9696
openstack-config --set /etc/nova/nova.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  neutron auth_type  password
openstack-config --set /etc/nova/nova.conf  neutron project_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron user_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  neutron project_name  service
openstack-config --set /etc/nova/nova.conf  neutron username  neutron
openstack-config --set /etc/nova/nova.conf  neutron password  NEUTRON_PASS

#重启compute的nova服务：
systemctl restart openstack-nova-compute.service
#启动并开机自启neutron服务：
systemctl enable neutron-linuxbridge-agent.service
systemctl start neutron-linuxbridge-agent.service
#验证网络服务（在controller节点验证）：
openstack network agent list

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
9，安装web页面
安装在controller节点：
此处有更改好的模板，直接将模板导入安装后的配置文件中。
安装：
yum install openstack-dashboard -y
配置：
cat local_settings >/etc/openstack-dashboard/local_settings （记录好模板）
sed -i '3a WSGIApplicationGroup %{GLOBAL}' /etc/httpd/conf.d/openstack-dashboard.conf（修改不能访问的bug）
#重启apache和memcached
systemctl restart httpd.service memcached
#访问：
http://ip/dashboard

域 default 账户：admin  密码：ADMIN_PASS

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
10，命令行创建模板：
#创建网络：（注意，此处会提示neutron的cli会被移除，这个命令还可以用）
neutron net-create --shared --provider:physical_network provider --provider:network_type flat WAN
#注意：下面ip替换成宿主机ip网段，用于平面网络的ip分配
neutron subnet-create --name subnet-wan --allocation-pool \
start=192.168.75.100,end=192.168.75.200 --dns-nameserver 223.5.5.5 \
--gateway 192.168.75.2 WAN 192.168.75.0/24
#生成硬件配置模板（也可以在web页面配置生产）：
openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano
#生成sshkey，并且作为controller节点链接虚拟机的默认注入密钥。
ssh-keygen -q -N "" -f ~/.ssh/id_rsa
openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
#生成默认安全组（安全组名字default）
openstack security group rule create --proto icmp default
openstack security group rule create --proto tcp --dst-port 22 default  

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
扩容按照前面再compute的操作全部操作一遍即可，如使用现成的配置文件scp的话注意配置文件权限问题。
如果开机报错：
PMD: net_mlx4: cannot load glue library: libibverbs.so.1: cannot open shared object file: No such file or directory
解决方法：
yum install -y libibverbs
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
cinder卷存储（此处选择扩容后的copmute1为存储节点，且增加了两块硬盘作为存储测试。）
compute1兼职存储卷节点ip为192.168.75.17（生产建议存储卷节点单独部署）


cinder安装：
在controller节点操作：
1：数据库创库授权
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
  IDENTIFIED BY 'CINDER_DBPASS';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
  IDENTIFIED BY 'CINDER_DBPASS';

2：在keystone创建系统用户(glance,nova,neutron,cinder)关联角色
openstack user create --domain default --password CINDER_PASS cinder
openstack role add --project service --user cinder admin

3：在keystone上创建服务和注册api
openstack service create --name cinderv2 \
  --description "OpenStack Block Storage" volumev2
openstack service create --name cinderv3 \
  --description "OpenStack Block Storage" volumev3
openstack endpoint create --region RegionOne \
  volumev2 public http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne \
  volumev2 internal http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne \
  volumev2 admin http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne \
  volumev3 public http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne \
  volumev3 internal http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne \
  volumev3 admin http://controller:8776/v3/%\(project_id\)s


4，装包配置
yum install openstack-cinder -y
cp /etc/cinder/cinder.conf{,.bak}
grep -Ev '^$|#' /etc/cinder/cinder.conf.bak >/etc/cinder/cinder.conf
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  transport_url rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  auth_strategy  keystone
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  my_ip  192.168.112.100
openstack-config --set /etc/cinder/cinder.conf   database connection mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_uri  http://controller:5000
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_url  http://controller:35357
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   memcached_servers  controller:11211
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_type  password
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   user_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_name  service
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   username  cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   password  CINDER_PASS
openstack-config --set /etc/cinder/cinder.conf   oslo_concurrency  lock_path  /var/lib/cinder/tmp
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  glance_api_servers  http://controller:9292

5：同步数据库
su -s /bin/sh -c "cinder-manage db sync" cinder

6：启动服务
openstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne
systemctl restart openstack-nova-api.service
systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service

验证：
openstack volume service list

##使用compute1节点兼职cinder存储节点（此处为块存储演示）：
++++++++++++++++++++++++++++++++++++++++++++++
yum install lvm2 device-mapper-persistent-data -y
systemctl enable lvm2-lvmetad.service
systemctl start lvm2-lvmetad.service
###增加两块硬盘
echo '- - -' >/sys/class/scsi_host/host0/scan (扫描识别硬盘)
fdisk -l
pvcreate /dev/sdb  创建pv
pvcreate /dev/sdc
vgcreate cinder-ssd /dev/sdb  创建vg
vgcreate cinder-sata /dev/sdc
###修改/etc/lvm/lvm.conf
在130下面插入一行：
filter = [ "a/sdb/", "a/sdc/","r/.*/"]

安装
yum install openstack-cinder targetcli python-keystone -y

配置
[root@compute1 ~]# cat /etc/cinder/cinder.conf
[DEFAULT]
transport_url = rabbit://openstack:RABBIT_PASS@controller
auth_strategy = keystone
my_ip = 192.168.75.17
glance_api_servers = http://controller:9292
enabled_backends = ssd,sata
[BACKEND]
[BRCD_FABRIC_EXAMPLE]
[CISCO_FABRIC_EXAMPLE]
[COORDINATION]
[FC-ZONE-MANAGER]
[KEYMGR]
[cors]
[cors.subdomain]
[database]
connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = CINDER_PASS
[matchmaker_redis]
[oslo_concurrency]
lock_path = /var/lib/cinder/tmp
[oslo_messaging_amqp]
[oslo_messaging_notifications]
[oslo_messaging_rabbit]
[oslo_middleware]
[oslo_policy]
[oslo_reports]
[oslo_versionedobjects]
[ssl]
[ssd]
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_group = cinder-ssd
iscsi_protocol = iscsi
iscsi_helper = lioadm
volume_backend_name = ssd
[sata]
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_group = cinder-sata
iscsi_protocol = iscsi
iscsi_helper = lioadm
volume_backend_name = sata


启动
systemctl enable openstack-cinder-volume.service target.service
systemctl start openstack-cinder-volume.service target.service
验证：
openstack volume service list
此时重新登录web界面会多出一个卷的选项即cinder卷服务。
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
openstack的vxlan网络：
配置说明：所有节点都开一个新网卡，设置网段为172.16.1.0/24的网段。
配置静态ip注意设置netmask为255.255.255.0
对应controller节点为：172.16.1.15
compute节点为：172.16.1.16
compute1节点为：172.16.1.17
在控制节点（controller）：
1，删除所有平面网络的实例
2，修改/etc/neutron/neutron.conf的[DEFAULT]区域
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
3，修改/etc/neutron/plugins/ml2/ml2_conf.ini文件
[ml2]区域修改如下
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = linuxbridge,l2population
在[ml2_type_vxlan]区域增加一行
vni_ranges = 1:1000
最终的配置文件如下
[root@controller ~]# cat /etc/neutron/plugins/ml2/ml2_conf.ini
[DEFAULT]
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = linuxbridge,l2population
extension_drivers = port_security
[ml2_type_flat]
flat_networks = provider
[ml2_type_geneve]
[ml2_type_gre]
[ml2_type_vlan]
[ml2_type_vxlan]
vni_ranges = 1:1000
[securitygroup]
enable_ipset = True


4，修改/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件

在[vxlan]区域下
修改为
enable_vxlan = True
local_ip = 172.16.1.15
l2_population = True

#172.16.1.15这个IP还没有，马上就配，主机规划的第二块网卡，就是现在用的
最终的配置文件如下
[root@controller ~]# cat /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[DEFAULT]
[agent]
[linux_bridge]
physical_interface_mappings = provider:ens33
[securitygroup]
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
[vxlan]
enable_vxlan = True
local_ip = 172.16.1.15
l2_population = True
#千万不要重启网卡!!!
#我们使用ifconfig命令来添加网卡
ifconfig eth1 172.16.1.15 netmask 255.255.255.0
或者ifup ens37（我这里网卡是ens37，请使用者按照自己网卡的具体情况操作）
#5，修改/etc/neutron/l3_agent.ini文件
#在[DEFAULT]区域下，增加
interface_driver = linuxbridge
#重启neutron服务
systemctl restart neutron-server.service  neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
#启动L3服务:
systemctl start neutron-l3-agent.service
#开机启动L3服务:
systemctl enable neutron-l3-agent.service


计算节点：
配置
修改/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件
在[vxlan]区域下
enable_vxlan = True
local_ip = 172.16.1.16
l2_population = True
最终的配置文件如下：
[root@compute ~]# cat /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[DEFAULT]
[agent]
[linux_bridge]
physical_interface_mappings = provider:ens33
[securitygroup]
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
[vxlan]
enable_vxlan = True
local_ip = 172.16.1.16
l2_population = True
#这个ip暂时没有，所以也需要配置
#千万不要重启网卡!!!
#我们使用ifconfig命令来添加网卡
ifconfig eth1 172.16.1.16 netmask 255.255.255.0
启动
重启agent服务
systemctl restart neutron-linuxbridge-agent.service

备注：compute1节点（即192.168.75.17）重复计算节点compute的步骤即可。

回到控制节点打开web界面的路由配置：
vim /etc/openstack-dashboard/local_settings
将263行的
'enable_router': False,
修改为
'enable_router': True,
重启httpd与memcache
systemctl restart httpd.service memcached.service
在dashboard上开启三层路由
只有修改了/etc/openstack-dashboard/local_settings，才能开启三层路由器
图形化操作步骤：
1，在管理员网络处增加vpc网络将原来的wan网设置成外部网络，vpc自建ip与子网
2，在web界面路由处创建路由，将路由与wan网段接起来。
3，到网络拓扑界面，在路由的图标上设置接口与vpc链接。
4，在实例界面生成实例，网络选择vpc
5，此时实例可以出外网，但是外网链接不进去。需要配置浮动ip
6，如果在实例界面分配不了浮动ip可以到管理员的网络界面先生成浮动ip再关联。








